Is cuda available ? False
TRAINING IMAGES SHAPE torch.Size([10000, 64, 64])
epoch: 0
0.0
images
Latent parameters:
tensor([[[ 1.3315e-02, -4.2823e-02,  3.2637e-02,  ..., -2.9925e-02,
           2.6306e-02,  8.8113e-03],
         [ 2.3531e-03,  1.3355e-02,  5.0779e-02,  ...,  1.2405e-02,
           2.3168e-02, -3.1721e-02],
         [ 1.0115e+00,  1.0337e+00,  1.0446e+00,  ...,  9.8429e-01,
           1.0027e+00,  1.0414e+00],
         [ 9.9576e-01,  1.0208e+00,  9.9646e-01,  ...,  1.0036e+00,
           9.7148e-01,  1.0297e+00]],
        [[ 1.9990e-02, -2.7877e-02,  2.8469e-02,  ..., -2.8199e-02,
           2.8870e-02,  2.4011e-03],
         [-2.1881e-02,  1.5129e-02,  4.7052e-02,  ...,  8.5703e-03,
           8.1977e-03, -3.4212e-02],
         [ 1.0135e+00,  1.0351e+00,  1.0396e+00,  ...,  9.7593e-01,
           1.0042e+00,  1.0322e+00],
         [ 9.9589e-01,  1.0348e+00,  1.0151e+00,  ...,  9.9917e-01,
           9.7271e-01,  1.0297e+00]],
        [[ 2.4946e-02, -3.8421e-02,  3.8768e-02,  ..., -3.8183e-02,
           2.2436e-02,  1.1470e-02],
         [-7.5230e-03,  1.8921e-02,  4.8642e-02,  ...,  1.3462e-02,
           1.3856e-02, -2.9691e-02],
         [ 1.0234e+00,  1.0395e+00,  1.0386e+00,  ...,  9.7670e-01,
           9.9813e-01,  1.0388e+00],
         [ 9.9949e-01,  1.0288e+00,  1.0055e+00,  ...,  1.0057e+00,
           9.7250e-01,  1.0344e+00]],
        ...,
        [[ 1.8607e-02, -3.6666e-02,  3.8131e-02,  ..., -3.1793e-02,
           2.5043e-02,  4.2837e-04],
         [-1.2332e-02,  1.9818e-02,  3.9840e-02,  ...,  1.2300e-02,
           1.3687e-02, -3.4284e-02],
         [ 1.0091e+00,  1.0331e+00,  1.0366e+00,  ...,  9.7000e-01,
           1.0025e+00,  1.0417e+00],
         [ 1.0068e+00,  1.0241e+00,  1.0040e+00,  ...,  1.0123e+00,
           9.7162e-01,  1.0442e+00]],
        [[ 2.1751e-02, -3.3695e-02,  3.1932e-02,  ..., -3.5839e-02,
           2.4014e-02,  6.7797e-03],
         [-1.1613e-02,  2.4255e-02,  4.2003e-02,  ...,  1.2748e-02,
           2.0874e-02, -3.1162e-02],
         [ 1.0117e+00,  1.0449e+00,  1.0504e+00,  ...,  9.7150e-01,
           1.0048e+00,  1.0326e+00],
         [ 1.0030e+00,  1.0208e+00,  9.9959e-01,  ...,  1.0065e+00,
           9.6190e-01,  1.0367e+00]],
        [[ 1.2624e-02, -3.6016e-02,  3.7532e-02,  ..., -2.6286e-02,
           2.6492e-02,  3.8291e-03],
         [-6.6600e-03,  2.2460e-02,  4.5552e-02,  ...,  1.2894e-02,
           1.5995e-02, -3.3283e-02],
         [ 1.0082e+00,  1.0395e+00,  1.0428e+00,  ...,  9.8694e-01,
           1.0042e+00,  1.0376e+00],
         [ 1.0013e+00,  1.0345e+00,  1.0040e+00,  ...,  1.0108e+00,
           9.7194e-01,  1.0233e+00]]], grad_fn=<ReshapeAliasBackward0>)
Time latent: 0.0875697135925293
/Users/gabdu45/PycharmProjects/segmentationCryoEM/imageRenderer.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  ctf = torch.sqrt(torch.tensor(1 - w ** 2, device=self.device)) * torch.sin(torch.tensor(gamma, device=self.device))\
/Users/gabdu45/PycharmProjects/segmentationCryoEM/imageRenderer.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  - w * torch.cos(torch.tensor(gamma, device=self.device))
/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py:69: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with autograd.detect_anomaly(check_nan=False):
Mask tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        ...,
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.]], grad_fn=<SoftmaxBackward0>)
RMSD: tensor(497.4848, grad_fn=<NegBackward0>)
Dkl: tensor(25.0466, grad_fn=<NegBackward0>)
DKLS: tensor(-7.2303, grad_fn=<NegBackward0>) tensor(-0., grad_fn=<NegBackward0>) tensor(-0., grad_fn=<NegBackward0>)
LOSS tensor(499.2811, grad_fn=<DivBackward0>)
LOSS GRAD None
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
STEP
Printing metrics
Running time one iteration: 14.529210090637207
epoch: 0
0.01
images
Latent parameters:
tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],
        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],
        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],
        ...,
        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],
        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]],
        [[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]],
       grad_fn=<ReshapeAliasBackward0>)
/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py:102: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)
  print("LOSS GRAD", loss.grad)
Traceback (most recent call last):
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py", line 211, in <module>
    experiment()
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py", line 206, in experiment
    train_loop(net, absolute_positions, renderer, local_frame)
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py", line 91, in train_loop
    = network.forward(batch_indexes, deformed_images)
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/network.py", line 250, in forward
    latent_variables, latent_parameters= self.sample_latent(indexes, images)
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/network.py", line 142, in sample_latent
    all_latent_axis = utils.sample_power_spherical(3, latent_mu_axis, latent_concentration_axis, device=self.device)
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/utils.py", line 237, in sample_power_spherical
    beta_distrib = torch.distributions.beta.Beta(alpha, beta, device)
  File "/Users/gabdu45/.local/lib/python3.10/site-packages/torch/distributions/beta.py", line 38, in __init__
    self._dirichlet = Dirichlet(concentration1_concentration0, validate_args=validate_args)
  File "/Users/gabdu45/.local/lib/python3.10/site-packages/torch/distributions/dirichlet.py", line 54, in __init__
    super(Dirichlet, self).__init__(batch_shape, event_shape, validate_args=validate_args)
  File "/Users/gabdu45/.local/lib/python3.10/site-packages/torch/distributions/distribution.py", line 56, in __init__
    raise ValueError(
ValueError: Expected parameter concentration (Tensor of shape (100, 4, 1, 2)) of distribution Dirichlet(concentration: torch.Size([100, 4, 1, 2])) to satisfy the constraint IndependentConstraint(GreaterThan(lower_bound=0.0), 1), but found invalid values:
tensor([[[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]]], grad_fn=<StackBackward0>)