Is cuda available ? False
TRAINING IMAGES SHAPE torch.Size([10000, 64, 64])
epoch: 0
0.0
images
Time latent: 0.03744792938232422
/Users/gabdu45/PycharmProjects/segmentationCryoEM/imageRenderer.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  ctf = torch.sqrt(torch.tensor(1 - w ** 2, device=self.device)) * torch.sin(torch.tensor(gamma, device=self.device))\
/Users/gabdu45/PycharmProjects/segmentationCryoEM/imageRenderer.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  - w * torch.cos(torch.tensor(gamma, device=self.device))
/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py:69: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with autograd.detect_anomaly(check_nan=False):
Mask tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        ...,
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.]], grad_fn=<SoftmaxBackward0>)
RMSD: tensor(nan, grad_fn=<NegBackward0>)
Dkl: tensor(25.0815, grad_fn=<NegBackward0>)
DKLS: tensor(-7.2303, grad_fn=<NegBackward0>) tensor(-0., grad_fn=<NegBackward0>) tensor(-0., grad_fn=<NegBackward0>)
LOSS tensor(nan, grad_fn=<DivBackward0>)
LOSS GRAD None
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
tensor(nan)
STEP
Printing metrics
Running time one iteration: 12.349447011947632
epoch: 0
0.01
images
/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py:102: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)
  print("LOSS GRAD", loss.grad)
Traceback (most recent call last):
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py", line 211, in <module>
    experiment()
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py", line 206, in experiment
    train_loop(net, absolute_positions, renderer, local_frame)
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/main.py", line 91, in train_loop
    = network.forward(batch_indexes, deformed_images)
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/network.py", line 249, in forward
    latent_variables, latent_parameters= self.sample_latent(indexes, images)
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/network.py", line 141, in sample_latent
    all_latent_axis = utils.sample_power_spherical(3, latent_mu_axis, latent_concentration_axis, device=self.device)
  File "/Users/gabdu45/PycharmProjects/segmentationCryoEM/utils.py", line 237, in sample_power_spherical
    beta_distrib = torch.distributions.beta.Beta(alpha, beta, device)
  File "/Users/gabdu45/.local/lib/python3.10/site-packages/torch/distributions/beta.py", line 38, in __init__
    self._dirichlet = Dirichlet(concentration1_concentration0, validate_args=validate_args)
  File "/Users/gabdu45/.local/lib/python3.10/site-packages/torch/distributions/dirichlet.py", line 54, in __init__
    super(Dirichlet, self).__init__(batch_shape, event_shape, validate_args=validate_args)
  File "/Users/gabdu45/.local/lib/python3.10/site-packages/torch/distributions/distribution.py", line 56, in __init__
    raise ValueError(
ValueError: Expected parameter concentration (Tensor of shape (100, 4, 1, 2)) of distribution Dirichlet(concentration: torch.Size([100, 4, 1, 2])) to satisfy the constraint IndependentConstraint(GreaterThan(lower_bound=0.0), 1), but found invalid values:
tensor([[[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]],
        [[[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]],
         [[nan, 1.]]]], grad_fn=<StackBackward0>)